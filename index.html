<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <link href="style.css" rel="stylesheet" />

    <title>Representation Learning for Chord Estimation</title>
    <meta
      property="og:title"
      content="Representation Learning for Chord Estimation"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Representation Learning for Chord Estimation</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_website">Natalie Huang</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Opalina Vetrichelvan</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#does_x_do_y">Does X do Y?</a><br /><br />
          <a href="#implications_and_limitations"
            >Implications and limitations</a
          ><br /><br />
        </div>
      </div>

      <!-- <div class="main-content-block"> -->
      <!--You can embed an image like this:-->
      <!-- <img src="./images/your_image_here.png" width=512px/> -->
      <!-- </div> -->
      <!-- <div class="margin-right-block">
					Caption for the image.
		</div> -->
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>
        Contrastive learning has demonstrated significant success in domains like computer vision and natural language processing by creating robust, generalized embeddings from unlabeled data. This method is especially valuable in the field of musical information retrieval (MIR) tasks, where annotating and labeling music and audio data is time-intensive, and the ratio of labeled to unlabeled data is heavily skewed. In this project, we wish to apply contrastive learning techniques to create musical embeddings and evaluate their utility in MIR tasks. 
        <br /><br />
        While musical embeddings derived from contrastive learning methods have been applied to high-level tasks such as classifying tags (e.g., genre, emotion, or artist), they have not yet been evaluated on frame-level tasks. Frame-level tasks, including chord estimation and beat tracking, differ from high-level tasks as they require embeddings that capture temporal information at a fine granularity. Our goal is to explore strategies for generating frame-level embeddings using contrastive learning, test these embeddings on sequence-to-sequence (seq2seq) tasks, and compare their performance to existing methods.

      </div>
      <div class="margin-right-block">Delete this</div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Background and Related Work</h1>
        
        Contrastive Learning was popularized through SimCLR in computer vision. In the audio domain, this framework has been extended, used in music [CLMR, MULE] and speech [Speech SimCLR], leading to competitive performance in music (genre classification, multi-class classification) and speech benchmarks (speech emotion recognition).
        <br /><br />
        The MARBLE Benchmark was specifically developed to probe and compare embeddings from a range of popular music models, including those derived from contrastive learning (e.g., CLMR and MULE) as well as models like Jukebox, MusicCNN, and the MAP-based family (MAP-Music2Vec, MAP-MERT). While MULE and Jukebox achieved leading performance on high-level tasks such as genre classification, they were not evaluated on seq2seq tasks like chord estimation and beat tracking, as they lack frame-level embeddings. In contrast, models from the MAP-based family provide frame-level embeddings, making them better suited for these tasks.
        <br /><br />
        Motivated by this gap, we aim to assess whether frame-level embeddings can be derived from contrastive learning methods. We will evaluate these embeddings on downstream seq2seq tasks, focusing on chord estimation, to compare their performance with existing methods.
        
        <h4>Contrastive Predictive Encoding</h4>
        <div class="content-margin-container">
          <div class="margin-left-block"></div>
          <div class="main-content-block">
            <img src="./images/cpc_architecture.png" width="512px" />
          </div>
          <div class="margin-right-block">
            Figure 1: Visualization of CPC architecture
          </div>
        </div>

        <br />
        We chose to use Contrastive Predictive Coding (CPC) to learn frame-level
        musical representations due to its similarity to SimCLR and
        compatibility with sequential data. Similar to SimCLR, CPC uses postivie
        and negative example pairs to learn representations. However, CPC also
        uses an autoregressive model to include the context of data sample in
        the latent representation. Formally, the CPC model consists of an
        encoder \( z_t = g_{enc}(x_t) \) that maps observation \(x_t\) to latent
        representation \(z_t\). Next, an autogressive model \(g_{ar}\) takes the
        existing sequence of \(z_t\) vectors to produce a latent representation
        of the context \(c_t\). Specifically, CPC tries to predict the next
        \(k\) samples. CPC minimizes a probabilistic contrastive loss, shown
        below.
        
      </div>
      <div class="margin-right-block">Delete this</div>
    </div>

    
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <img src="./images/cpc_loss.png" width="300px" />
      </div>
      <div class="margin-right-block">
        Equation 1: probabilistic contrastive loss
      </div>
    </div>

    

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Methods and Experiments</h2>
        The contrastive learning model we picked to derive frame level embeddings was Jaka to Melody (JTM) which utilizes contrastive predictive encoding (CPC). We also wished to use embeddings from Jukebox as a comparison, which is a popular music generation model developed by Open AI. Jukebox embeddings were chosen as they are a good non-contrastive method to compare our results, and evaluating them fills a gap in knowledge as they were also not assessed on frame-level tasks in the MARBLE benchmark.  
      
      
      </div>
      <div class="margin-right-block"></div>
    </div>
    

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>CPC Experiments</h2>
        Our base implementation of CPC was based off
        <a
          href="https://github.com/tiagoCuervo/JTM/tree/main"
          target="_blank"
          rel="noopener noreferrer"
        >
          this repository </a
        >from the JTM project. We used a convolutional SincNet encoder for
        \(g_{enc}\) and a simple GRU for \(g_{ar}\), which were architectures
        that worked for the experiments in JTM. \(k\) for the autoregressive was
        \(12\)
      </div>
      <div class="margin-right-block">
        Equation 1: probabilistic contrastive loss
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Dataset</h1>
        The original paper by van den Oord et. al is trained on an audio speech
        dataset. We chose to train our encoder on
        <a
          href="https://www.kaggle.com/datasets/imsparsh/musicnet-dataset"
          target="_blank"
          rel="noopener noreferrer"
        >
          MusicNet
        </a>
        because we were interested in the downstream tasks of music
        transcription and chord estimation. MusicNet provides raw wav files of
        classical music recordings, annotated with note start and end times for
        each instrument. We trained our encoder on MusicNet exclusively, and
        tested it on downstream tasks using both MusicNet and other datasets, to
        test generalizability to other genres.
        <br />
        <br />
        For preprocessing, each file was downsampled to 16kHz instead of
        44.1kHz, to meet memory and compute limitations. We did not change the
        window sizes of the implementation, which spanned \(20480\) frames or
        \(1.28\) seconds of audio each. Each window of data was represented by
        \(128\) latent vectors, so each latent vector spanned \(10\) ms of
        audio.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Results</h2>
        <h1>Encoder Training</h1>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div
        class="main-content-block"
        style="
          display: flex;
          gap: 10px;
          justify-content: center;
          align-items: center;
        "
      >
        <div>
          <img src="./images/encoder_training/big_train_acc1.png" />
          <!-- Training acc for prediction step \(t = 0\) (yaxis scale from 0 - 1) -->
        </div>
        <div>
          <img src="./images/encoder_training/big_train_acc5.png" />
          <!-- Training acc for prediction \(t = 5\) -->
        </div>
        <!-- <img src="./images/encoder_training/train_acc_1.jpeg" width="300" /> -->
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div
        class="main-content-block"
        style="
          display: flex;
          gap: 10px;
          justify-content: center;
          align-items: center;
        "
      >
        <div>
          <img src="./images/encoder_training/big_train_acc10.png" width="50%" />
          <!-- Training acc for prediction for \(t = 10\) -->
        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        As expected, training accuracy drops as \(t\) increases.
      </div>
      <div class="margin-right-block"></div>
    </div>

	<div class="content-margin-container">
		<div class="margin-left-block"></div>
		<div class="main-content-block">
		  <h2>Downstream Tasks</h2>
		  <h1>Note Transcription</h1>
		</div>
		<div class="margin-right-block"></div>
	  </div>
    <div class="content-margin-container" id="does_x_do_y">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Does X do Y?</h1>
        It is well known that Y does Y. And this has raised the question does X
        do Y? Because if Y does Y then it stands to reason that X does Y. But we
        cannot answer this until we realize the Z implies Y and X can be linked
        to Z.<br /><br />

        Now let's write some math!<br />
        <center>
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mrow>
                <mo>&#x2202;</mo>
                <mi>y</mi>
              </mrow>
              <mo>/</mo>
              <mrow>
                <mo>&#x2202;</mo>
                <mi>x</mi>
              </mrow>
            </mrow>
            <mo>=</mo>
            <mi>x</mi>
          </math>
        </center>
        <br />
        It's probably best to ask an LLM to help do the web formatting for math.
        You can tell it "convert this latex equation into MathML:
        $$\frac{\partial dy}{\partial dx} = x$$" But it took me a few tries. So,
        if you get frustrated, you can embed an image of the equation, or use
        other packages for rendering equations on webpages.
      </div>
      <div class="margin-right-block" style="transform: translate(0%, -100%)">
        <!-- you can move the margin notes up and down with translate -->
        Interestingly, Plato also asked if X does Y, in
        <a href="#ref_1">[1]</a>.
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Background and Related Work</h1>
        In this section we embed a video:
        <video class="my-video" loop autoplay muted style="width: 725px">
          <source src="./images/mtsh.mp4" type="video/mp4" />
        </video>
      </div>
      <div class="margin-right-block">
        A caption for the video could go here.
      </div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave"
            >Allegory of the Cave</a
          >, Plato, c. 375 BC<br /><br />
          <a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI,
          2025<br /><br />
        </div>
      </div>
      <div class="margin-right-block">
        <!-- margin notes for reference block here -->
      </div>
    </div>
  </body>
</html>
