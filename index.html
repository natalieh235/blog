<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script
      type="text/javascript"
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <link href="style.css" rel="stylesheet" />

    <title>Representation Learning for Chord Estimation</title>
    <meta
      property="og:title"
      content="Representation Learning for Chord Estimation"
    />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Representation Learning for Chord Estimation</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_website">Natalie Huang</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Opalina Vetrichelvan</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <!-- CITE WITH THIS <a href="#ref_1">[1]</a> -->
    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Intro</a><br /><br />
          <a href="#bg">Background</a><br /><br />
          <a href="#methods">Methods</a><br /><br />
          <a href="#results">Results</a><br /><br />
          <a href="#conclusion">Conclusion</a><br /><br />
        </div>
      </div>

      <!-- <div class="main-content-block"> -->
      <!--You can embed an image like this:-->
      <!-- <img src="./images/your_image_here.png" width=512px/> -->
      <!-- </div> -->
      <!-- <div class="margin-right-block">
					Caption for the image.
		</div> -->
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Introduction</h2>
        Contrastive learning has demonstrated significant success in domains
        like computer vision and natural language processing by creating robust,
        generalized embeddings from unlabeled data. This method is especially
        valuable in the field of musical information retrieval (MIR) tasks,
        where annotating and labeling music and audio data is time-intensive,
        and the ratio of labeled to unlabeled data is heavily skewed. In this
        project, we wish to apply contrastive learning techniques to create
        musical embeddings and evaluate their utility in MIR tasks.
        <br /><br />
        While musical embeddings derived from contrastive learning methods have
        been applied to high-level tasks such as classifying tags (e.g., genre,
        emotion, or artist), they have not yet been evaluated on frame-level
        tasks. Frame-level tasks, including chord estimation and beat tracking,
        differ from high-level tasks as they require embeddings that capture
        temporal information at a fine granularity. Our goal is to explore
        strategies for generating frame-level embeddings using contrastive
        learning, test these embeddings on sequence-to-sequence (seq2seq) tasks,
        and compare their performance to existing methods.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="bg">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Background</h2>

        Contrastive Learning was popularized through SimCLR <a href="#ref_1">[1]</a> 
        in computer vision. In the audio domain, this framework has been
        extended, used in music [CLMR <a href="#ref_2">[2]</a>  , MULE <a href="#ref_3">[3]</a>] and speech [Speech SimCLR <a href="#ref_4">[4]</a>], leading
        to competitive performance in music and speech benchmarks.
        <br /><br />

        The MARBLE Benchmark (Music Audio Representation Benchmark for Universal
        Evaluation <a href="#ref_4">[4]</a>  ) was specifically developed to probe and
        compare embeddings from a range of popular music models, including those
        derived from contrastive learning (e.g., CLMR and MULE) as well as
        models like Jukebox, MusicCNN, and the MAP-based family (MAP-Music2Vec,
        MAP-MERT). While MULE and Jukebox achieved leading performance on
        high-level tasks such as genre classification, they were not evaluated
        on seq2seq tasks like chord estimation and beat tracking, as they lack
        frame-level embeddings. In contrast, models from the MAP-based family
        provide frame-level embeddings, making them better suited for these
        tasks. <br /><br />

        Li et al. 2023 <a href="#ref_6">[6]</a> identifies that there exists a large gap between
        understanding and evaluation of ‘clip-level’ vs ‘frame-level’ MIR tasks,
        where clip level tasks classify the acoustic scene of an entire audio
        clip and ‘frame-level’ tasks must detect and account for time-stamped
        events. They address this by developing two versions of an Audio
        Teacher-Student Transformer model (ATST), with a clip-level version
        (named ATST-Clip) and a frame-level version (named ATST-Frame).
        Evaluation of contrastive learning methods on frame-level tasks however
        has not been thoroughly explored. Additionally, having one model able to
        perform both clip-level and frame-level tasks would be more desirable,
        so that it can capture meaningful clip level and frame level musical
        aspects at the same time.<br /><br />
        Motivated by this gap, we aim to assess whether frame-level embeddings
        can be derived from contrastive learning methods. We will evaluate these
        embeddings on downstream seq2seq tasks, focusing on chord estimation, to
        compare their performance with existing methods. <br /><br />

        <h1>Related Work</h1>
        Here, we give a brief overfiew of models used in our experiments.
        <h4>Contrastive Predictive Coding</h4>
        <div class="content-margin-container">
          <div class="margin-left-block"></div>
          <div class="main-content-block">
            <img src="./images/cpc_architecture.png" width="512px" />
          </div>
          <div class="margin-right-block">
            Figure 1: Visualization of CPC architecture
          </div>
        </div>

        <br /><br />
        We chose to use Contrastive Predictive Coding (CPC) <a href="#ref_7">[7]</a> to learn frame-level
        musical representations due to its similarity to SimCLR and
        compatibility with sequential data. CPC uses positive and negative
        example pairs to learn representations, as well as an autoregressive
        model to include the context of data sample in the latent
        representation. Formally, the CPC model consists of an encoder \( z_t =
        g_{enc}(x_t) \) that maps observation \(x_t\) to latent representation
        \(z_t\). Next, an autogressive model \(g_{ar}\) takes the existing
        sequence of \(z_t\) vectors to produce a latent representation of the
        context \(c_t\). Specifically, CPC tries to predict the next \(k\)
        samples. CPC minimizes a probabilistic contrastive loss, shown below.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <img src="./images/cpc_loss.png" width="300px" />
      </div>
      <div class="margin-right-block">
        Equation 1: probabilistic contrastive loss
      </div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Jukebox</h1>
        JukeBox <a href="#ref_8">[8]</a> is a music generation model developed by
        Open-AI to address handling long-range dependencies in music, as a
        typical song can have over 10 million timesteps. They leverage a VQ-VAE
        <a href="#ref_9">[9]</a>, a method originally used on images to compress
        audio to a lower dimensional space. They use an autoregressive sparse
        transformer in this space and autoregressive up samplers to recreate
        lost information at different compression levels. JukeBox has been shown
        to generate songs from a diverse array of genres and can generate novel
        completions to music. In the MARBLE Benchmark however, they have yet
        been used to assess frame-level MIR tasks like beat tracking, and chord
        estimation. Since JukeBox is able to capture many musical aspects needed
        for music generation, this will serve as a competitive comparison to our
        embeddings derived from CPC.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="methods">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Methods and Experiments</h2>
        As an overview, we wanted to explore how different music embeddings
        performed on seq2seq downstream tasks. We explore embeddings generated
        by two different encoders, one with CPC and one with the Jukebox
        encodings, and evaluate their performance on two downstream tasks:
        automatic transcription and chord estimation.
        <br />
        The contrastive learning model implementation we picked to derive frame
        level embeddings was from the Jaka to Melody (JTM) repository, which
        utilizes contrastive predictive encoding (CPC). We also wished to use
        embeddings from Jukebox as a comparison, which is a popular music
        generation model developed by Open AI. Jukebox embeddings were chosen as
        they are a good non-contrastive method to compare our results, and
        evaluating them fills a gap in knowledge as they have not yet been
        assessed on frame-level tasks in the MARBLE benchmark.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Encoders</h2>
        <h1>CPC Encoder</h1>
        Our base implementation of CPC was based off
        <a
          href="https://github.com/tiagoCuervo/JTM/tree/main"
          target="_blank"
          rel="noopener noreferrer"
        >
          this repository </a
        >from the JTM project. We used a convolutional SincNet encoder for
        \(g_{enc}\) and a simple GRU for \(g_{ar}\), which were architectures
        that worked for the experiments in JTM. \(k\) for the autoregressive
        model was \(12\). Negative samples were uniformly sampled from audio
        files. Other hyperparamters were as follows: <br />
        Learning rate: \(2e-4\) <br />
        Optimizer: Adam <br />
        Epochs: 30 <br />
        Batch size: 8 <br />
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Dataset</h1>
        The original paper by van den Oord et. al is trained on an audio speech
        dataset. We chose to train our encoder on
        <a
          href="https://www.kaggle.com/datasets/imsparsh/musicnet-dataset"
          target="_blank"
          rel="noopener noreferrer"
        >
          MusicNet
        </a>
        because we were interested in the downstream tasks of music
        transcription and chord estimation. MusicNet provides raw wav files of
        classical music recordings, annotated with note start and end times for
        each instrument. We trained our encoder on MusicNet exclusively, and
        tested it on downstream tasks using both MusicNet and other datasets, to
        test generalizability to other genres.
        <br />
        <br />
        For preprocessing, each file was downsampled to 16kHz instead of
        44.1kHz, to meet memory and compute limitations. We did not change the
        window sizes of the implementation, which spanned \(20480\) frames or
        \(1.28\) seconds of audio each. Each window of data was represented by
        \(128\) latent vectors, so each latent vector spanned \(10\) ms of
        audio.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Jukebox Embeddings</h1>
        We were able to find Jukebox embeddings for the GuitarSet dataset only,
        and used this later on in the chord estimation task.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Downstream Tasks</h2>
        <h1>Music Transcription</h1>
        We evaluated performance on music transcription with embeddings
        generated by the CPC Model. <br /><br />
        <b>CPC Embeddings</b> <br />
        The first downstream task we wanted to try was automatic music
        transcription, a basic seq2seq task. We train a very basic MLP network
        that takes in embeddings of audio clips and generates the notes present
        in that clip. The embeddings we use are the context embeddings generated
        by the autoregressive model in CPC, which should encode both the actual
        data and its context. The dataset we used for this task was the same one
        we used to train the encoder, MusicNet. However, we focused only on
        samples that were played by Solo Piano, to reduce complexity. <br />
        To test these embeddings on a downstream task, we utilize a
        transcription window, which pools the embeddings together for analysis.
        This window aligns the embeddings with the correct transcription labels,
        ensuring that the temporal relationships between the embeddings and the
        ground truth are preserved. A diagram illustrating the different sizes
        and components involved in this workflow is shown below. One parameter
        we varied in our experiments is the size of the transcription window.
        Adjusting the window size can impact performance, as larger windows pool
        more information but may sacrifice temporal resolution, while smaller
        windows retain finer temporal details but may dilute the contextual
        signal. We tested different transcription window sizes but found it
        didn't not impact performance that heavily in our experiments. <br />
        The label for each transcription window is a one-hot encoding with size
        129, which represents 128 possible MIDI notes + silence. <br />
        The MLP network we used had a pooling layer to average the embeddings
        across the transcription window and a hidden layer with 100 units. We
        ran out of time, but wanted to test different architectures here as
        well. The loss function was weighted BCEWithLogitsLoss. Because
        transcription labels are very sparse and silent examples dominate, the
        loss is weighted by the ratio of negative to ratio of positive examples,
        to encourage predicting active notes.

        <img src="./images/window.jpeg" />
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Chord Estimation</h1>
        The second downstream task we wanted to try was chord estimation, which
        tries to generate a sequence of played chords for a music audio file. We
        were originally interested in this task because it was mentioned in the
        MARBLE benchmarks, but there was almost no data on how models performed
        on this task. We also wanted to explore if the CPC Encoder could
        generalize to music it wasn't trained on, so we used a different
        dataset. <br /><br />

        <b>GuitarSet</b> <br />
        We used the
        <a
          href="https://guitarset.weebly.com/"
          target="_blank"
          rel="noopener noreferrer"
        >
          GuitarSet</a
        >
        dataset for the chord estimation task. GuitarSet<a href="#ref_10">[10]</a> comprises of 360 guitar
        recording excerpts with detailed annotations, spanning five musical
        styles—Rock, Singer-Songwriter, Bossa Nova, Jazz, and Funk—and three
        chord progressions: 12-Bar Blues, Autumn Leaves, and Pachelbel Canon.
        The dataset provides raw audio files accompanied by metadata in the form
        of JAMS files. Each JAMS file describes every chord played, including
        its start time and duration. Below is a visualization of the JAMS file
        data for one audio file from the dataset. <br /><br />

        For labelling, we decided to restrict the set of possible chords to
        those used in the MARBLE benchmark. A subset is shown below.

        <img src="./images/chord_vocab.png" />
        All possible combinations of root notes and chord types added up to 420
        possible chords. <br /><br />

        <b>CPC Embeddings</b> <br />
        The classifier architecture started out the same as our classifier for
        music transcription. To improve performance, we made a couple changes
        that will be discussed in results. <br /><br />

        <b>Jukebox Embeddings</b> <br />
        The process for extracting GuitarSet Jukebox embeddings was based on the
        method described in Castellon et al. (2021)<a href="#ref_11">[11]</a>, with modifications inspired
        by Spotify's Llark paper <a href="#ref_12">[12]</a>. Specifically, we used the output from the 36th
        layer of the Jukebox encoder. The original Jukebox encoding produces a
        4800-dimensional vector sampled at 345 Hz. To preprocess the audio, we
        chunked it into 25-second clips—the maximum input length for Jukebox.
        The embeddings were mean-pooled using a transcription window of 100 ms,
        resulting in a downsampled frequency of 10 Hz and an embedding size of
        approximately 1.2 million for each 25-second audio clip, with a final
        dimension of [240, 4800]. A custom dataloader was implemented to align
        the chord notations from the JAM files with the corresponding
        embeddings, accounting for the transcription window as well as the start
        and duration times in the JAM files. The dataset was heavily imbalanced,
        as most embeddings corresponded to silence, labeled as 'N'. The figure
        below illustrates the distribution of chord classes across all embedding
        windows.
        <img src="./images/chord_frequencies.jpeg" />
        <br /><br />
        For the downstream task, we used the same 100-layer MLP architecture
        that was used to evaluate JTM. To address dataset imbalance, we applied
        a weighted BCE loss function similar to the one used in JTM.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="results">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Results</h2>
        <h1>Encoder Training</h1>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <img src="./images/encoder_training/allTrainAcc.jpeg" />
        <img src="./images/encoder_training/allTrainLoss.jpeg" />
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        The model overall seems to converge and can discriminate between
        positive and negative examples. As expected, accuracy is lower when
        making predictions farther into the future. In the legend, the last
        number indicates how many time steps ahead the autoregressive model is
        predicting.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Downstream Tasks</h2>
        <h1>Automatic Note Transcription</h1>
        We used embeddings from the CPC Model on the downstream task of note
        transcription.
        <img src="./images/music_transcription_jtm/classifierTrainLoss.jpeg" />
        <img src="./images/music_transcription_jtm/classTrainAcc.jpeg" />
        Originally, the results looked quite good but then we tried visualizing
        the predicted notes against the labels.
        <img src="./images/music_transcription_jtm/class_note_vis.png" />
        The reddish lines indicate the ground-truth labeled notes, and the gray
        lines are predictions. We guessed that the classifier architecture was
        probably not complex enough to achieve good test performance. We wanted
        to test an LSTM, but ran out of time.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Chord Estimation</h1>
        <b>CPC Embeddings</b> <br />
        We found that the CPC embeddings did not work well for chord estimation,
        especially since the encoder was not trained on GuitarSet. Additionally,
        we found again that the classifier architecture did not really suit the
        task.
        <img src="./images/chord_jtm/chord_train_acc.jpeg" />
        The train accuracy was not bad in total, but the purple line represents
        the accuracy when only considering active notes- predicting silence does
        not count, and this maximally achieves 10% accuracy.
        <img src="./images/chord_jtm/chord_val_acc.jpeg" />
        The validation accuracy was worse and also suggested that the model was
        overfitting- we adjusted the number of units in the hidden layer to 25
        and added dropout, but still found that the validation accuracy would
        start going down after a couple epochs. <br /><br />

        We additionally visualized some of the chords, where the red lines are
        labels and the grayish ones are predictions: <br />
        <div
          style="
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 10px;
          "
        >
          <img src="./images/chord_jtm/graph1.png" alt="Graph 1" width="50%" />
          <img src="./images/chord_jtm/graph2.png" alt="Graph 2" width="50%" />
        </div>
        We explored different transcription window sizes, but found that
        performance was roughly the same. <br /><br /><br />

        <b>Jukebox</b> <br />
        In the Jukebox downstream chord estimation task, we also calculated the
        two separate accuracies mentioned above: total prediction accuracy and
        non-silence prediction accuracy. The latter measures performance only on
        samples where chords are actually being played, which naturally yielded
        lower values than total accuracy. This is because silent frames were the
        most represented class in our data, and although the model could
        discriminate well when there was silence, it had a much lower accuracy
        when only looking at music playing frames. Over the course of training,
        both train and validation loss decreased while accuracy increased, as
        shown in the loss and accuracy curves below. We trained the model for 10
        epochs but observed that the accuracy had not yet plateaued, indicating
        potential for further improvement with more computational resources and
        extended training time. After 10 epochs, the model achieved the
        following accuracies: <br />
        Test Accuracy (all chord labels): \(0.537\) <br />
        Test Accuracy (non-silence labels only): \(0.247\) <br />
        We visualized the model's predictions on the test set by plotting
        predicted vs. actual chords for the first three seconds of test audio
        data (225 embeddings). <br />

        <img src="./images/jukebox_res.jpeg" />
        This smaller window was chosen to better visualize patterns in the
        predicted chords. The actual chords often remained consistent across
        multiple frames, but the model treated each frame independently, failing
        to capture temporal continuity. This observation suggests that a
        different probing mechanism—such as an LSTM instead of an MLP—might
        better capture the temporal dynamics of chord transitions. With more
        time and resources, experimenting with such approaches could yield
        improved results. <br /><br />
        The JukeBox embeddings ended up achieving higher accuracy and lower loss
        than JTM. However there is some limitation to this comparison as the
        embeddings went through different pre-processing steps. JukeBox took
        input in 25 second chunks and was pooled slightly differently than JTM.
        They were also downsampled at different values. With more time, we could
        put in more effort to control for these parameters for a closer
        comparison of the embeddings.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="conclusion">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Conclusion</h1>
        This project demonstrates that embeddings generated from contrastive
        learning methods, such as CPC, effectively capture frame-level and
        temporal information. While JukeBox achieved higher accuracy, it is a
        significantly larger model with many more parameters compared to JTM.
        <br /><br />
        One limitation of this study was the dominance of silent frames in the
        dataset. With additional time and resources, re-running these models on
        filtered data with fewer silent frames could improve performance and
        provide a clearer understanding of the embeddings’ capacity to
        differentiate between chords. While using a separate accuracy metric for
        non-silence frames and a weighted loss function helps address this
        imbalance, less class imbalance might allow the model to better adjust
        its weights for discriminating chord classes. <br /><br />Exploring
        alternative probing mechanisms, such as an LSTM instead of an MLP, could
        help us investigate whether the embeddings effectively capture temporal
        patterns. Since CPC looks at future predictions to calculate embeddings,
        it should theoretically encode temporal information, but a classifier
        head more suited to making use of this information would probably
        improve performance. <br /><br />In conclusion, this work highlights the
        potential of contrastive learning in capturing complex temporal and
        contextual information in audio data. Future efforts can build on these
        findings by improving dataset balance, testing more sophisticated
        probing methods, and designing evaluation tasks tailored to temporal
        understanding. These directions hold promise for advancing both the
        interpretability and utility of contrastive learning methods in audio
        representation tasks.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://arxiv.org/abs/2002.05709"
            >A Simple Framework for Contrastive Learning of Visual
            Representations</a
          >, Ting Chen et al, 2020<br /><br />

          <a id="ref_2"></a>[2]
          <a href="https://archives.ismir.net/ismir2021/paper/000084.pdf"
            >CONTRASTIVE LEARNING OF MUSICAL REPRESENTATIONS (CLMR)</a
          >, Spijkervet et al, 2020<br /><br />

          <a id="ref_3"></a>[3]
          <a href="https://arxiv.org/abs/2210.03799"
            >Supervised and Unsupervised Learning of Audio Representations for
            Music Understanding (MULE)</a
          >, McCalllum et al, 2022<br /><br />

          <a id="ref_4"></a>[4]
          <a href="https://arxiv.org/pdf/2010.13991"
            >Speech SimCLR: Combining Contrastive and Reconstruction Objective
            for Self-supervised Speech Representation Learning </a
          >, Jiang et al, 2021<br /><br />

          <a id="ref_5"></a>[5]
          <a href="https://arxiv.org/pdf/2306.10548"
            >MARBLE: Music Audio Representation Benchmark for Universal
            Evaluation </a
          >, Yuan et al, 2023<br /><br />

          <a id="ref_6"></a>[6]
          <a href="https://arxiv.org/pdf/2306.04186"
            >Self-supervised Audio Teacher-Student Transformer for Both
            Clip-level and Frame-level Tasks </a
          >, Li et al, 2023<br /><br />

          <a id="ref_7"></a>[7]
          <a href="https://arxiv.org/pdf/1807.03748"
            >Representation Learning with Contrastive Predictive Coding </a
          >, DeepMind, 2019<br /><br />

          <a id="ref_8"></a>[8]
          <a href="https://arxiv.org/pdf/2005.00341"
            >Jukebox: A Generative Model for Music </a
          >, OpenAI, 2020<br /><br />

          <a id="ref_9"></a>[9]
          <a href="https://arxiv.org/pdf/2005.00341"
            >Generating Diverse High-Fidelity Images with VQ-VAE-2 </a
          >, Razavi et al, 2019<br /><br />

          <a id="ref_10"></a>[10]
          <a
            href="https://guitarset.weebly.com/uploads/1/2/1/6/121620128/xi_ismir_2018.pdf"
            >GUITARSET: A DATASET FOR GUITAR TRANSCRIPTION </a
          >, Xi et al, 2018<br /><br />

          <a id="ref_11"></a>[11]
          <a href="https://arxiv.org/abs/2107.05677"
            >Codified audio language modeling learns useful representations for
            music information retrieval </a
          >, Castellon et al, 2021<br /><br />

          <a id="ref_12"></a>[12]
          <a href="https://arxiv.org/pdf/2310.07160"
            >Llark: A Multimodal Instruction-Following Language Model for Music </a
          >, Spotify et al, 2024<br /><br />
        </div>
      </div>
      <div class="margin-right-block">
        <!-- margin notes for reference block here -->
      </div>
    </div>
  </body>
</html>
